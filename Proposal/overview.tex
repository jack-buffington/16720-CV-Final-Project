\section{Project Overview}

3D scene reconstruction is useful in robotics for understanding a robot's environment can aid in navigation through this environment. One typical approach is to use a combination of a GPS, laser scanners, and inertial measurement units to generate a map of the environment. However, this requires extensive, expensive, and specialized hardware. We would like to reconstruct a scene using video recorded from a standard cell phone camera. \\

The goal of this project is to reconstruct a 3D scene using video recorded from a standard cell phone camera. This project will attempt to recreate the work of Newcombe and Davison (\citep{newcombe2010live}). They present an algorithm that efficiently generates a dense model of a camera scene. It leverages a structure from motion algorithm called ``Parallel Tracking and Mapping'' (PTAM) to extract a high density of keypoints from each frame of the video feed. Their algorithm uses these points to first construct a low resolution model of the environment. Using groups of camera views with overlapping surface visibility, the model coarse model is refined to yield a dense depth map. Once this depth map is created for various scenes in the environment, the individual depth maps are stitched together to create a 3D map of the entire environment. \\

Other papers that address similar problems and which we may refer to include \citep{newcombe2010live}, \citep{davison2003real}, \citep{klein2007parallel}, \citep{pollefeys2008detailed}, \citep{kuhl2006monocular}, \citep{magree2014monocular}, \citep{geiger2011stereoscan}, \citep{mclauchlan2000batch}, and \citep{davison2007monoslam}.
